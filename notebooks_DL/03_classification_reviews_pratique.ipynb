{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification des critiques de films: un exemple de classification binaire\n",
    "\n",
    "La classification à deux classes, ou classification binaire, peut être le type de problème d’apprentissage automatique le plus largement appliqué. Dans cet exemple, nous apprendra à classer les critiques de films en critiques \"positives\" et \"négatives\", uniquement en fonction du contenu textuel des critiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The IMDB dataset\n",
    "\n",
    "Nous allons travailler avec \"l'ensemble de données IMDB\", un ensemble de 50 000 critiques de la base de données Internet Movie. Ils sont divisés en 25 000 revues pour l'apprentissage et 25 000 revues pour les tests, chaque série comprenant 50% de critiques négatives et 50% de critiques positives.\n",
    "\n",
    "Tout comme le jeu de données MNIST, le jeu de données IMDB est fourni avec Keras. \n",
    "\n",
    "Il a déjà été prétraité: les critiques (séquences de mots) ont été transformés en séquences d’entiers, chaque entier représentant un mot spécifique dans un dictionnaire.\n",
    "\n",
    "Le code suivant chargera le jeu de données (lorsque vous l'exécuterez pour la première fois, environ 80 Mo de données seront téléchargés sur votre ordinateur):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- `num_words = 10000` signifie que nous ne conserverons que les 10 000 mots les plus fréquents dans les données d'apprentissage. Cela nous permet de travailler avec des données vectorielles de taille gérable.\n",
    "\n",
    "- `train_data` et` test_data` sont des listes de revues, chaque revue étant une liste d'index de mots (codant une séquence de mots). \n",
    "- `train_labels` et` test_labels` sont des listes de 0 et de 1, où 0 signifie \"négatif\" et 1, \"positif\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Voici comment décoder une revue :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# word_index is a dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()\n",
    "# We reverse it, mapping integer indices to words\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# We decode the review; note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Préparation des données\n",
    "\n",
    "\n",
    "Nous ne pouvons pas introduire de listes d'entiers dans un réseau de neurones. Nous devons transformer nos listes en tenseurs. Nous pouvons le faire de deux manières :\n",
    "\n",
    "- Nous pourrions compiler nos listes pour qu’elles aient toutes la même longueur et les transformer en un tenseur entier de forme `(samples, word_indices)`, puis utiliser comme première couche de notre réseau une couche capable de gérer de tels tenseurs entiers (la couche `Embedding`).\n",
    "\n",
    "- Nous pourrions coder à chaud nos listes pour les transformer en vecteurs de 0 et de 1. Concrètement, cela signifierait, par exemple, tourner la séquence `[3, 5]` dans un vecteur de 10 000 dimensions qui serait composé de zéros sauf les indices 3 et 5, qui seraient un. Ensuite, nous pourrions utiliser comme première couche de notre réseau, une couche `Dense`, capable de traiter des données vectorielles à virgule flottante.\n",
    "\n",
    "Nous utilisons cette dernière solution. Vectorisons nos données, ce que nous ferons manuellement pour une clarté maximale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On vectorise aussi nos labels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Our vectorized labels\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Construire notre réseau\n",
    "\n",
    "Nos données d'entrée sont simplement des vecteurs et nos étiquettes sont des scalaires (1 et 0): c'est la configuration la plus simple. \n",
    "\n",
    "Un réseau qui fonctionne bien sur un tel problème serait une simple pile de couches entièrement connectées (`Dense`) avec les activations `relu`: `Dense (16,activation = 'relu')`\n",
    "\n",
    "L'argument transmis à chaque couche `Dense` (16) est le nombre d'unités cachées de la couche. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercice :** Implémentez ce réseau avec tensorflow et keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Enfin, nous devons choisir une fonction de perte et un optimiseur. \n",
    "\n",
    "Puisque nous sommes confrontés à un problème de classification binaire et à la sortie de notre réseau est une probabilité, il est préférable d'utiliser la perte `binary_crossentropy`.\n",
    "\n",
    "Ce n'est pas le seul choix viable: vous pouvez utiliser, par exemple, `mean_squared_error`. Mais la cross-centropie est généralement le meilleur choix lorsque vous traitent des modèles qui produisent des probabilités. \n",
    "\n",
    "Voici l'étape où nous configurons notre modèle avec l'optimiseur `rmsprop` et la fonction de perte` binary_crossentropy`. Notez que nous allons surveillez également la précision pendant l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Valider notre approche\n",
    "\n",
    "Afin de contrôler au cours de l'apprentissage la précision du modèle sur des données qu’il n’a jamais vues auparavant, nous allons créer un \"jeu de validation\" avec 10 000 échantillons des données d'entraînement originales :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x_val = ___\n",
    "partial_x_train = ___\n",
    "\n",
    "y_val = ___\n",
    "partial_y_train = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nous allons maintenant apprendre notre modèle pour 20 époques (20 itérations sur tous les échantillons des tenseurs `x_train` et` y_train`), en mini-lots de 512 échantillons. \n",
    "\n",
    "Parallèlement, nous surveillerons la perte et la précision des 10 000 échantillons que nous avons mis de côté. Ceci est fait en passant les données de validation en tant qu'argument `validation_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "history = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut afficher quelques viz :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.clf()   # clear figure\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Les points représentent la perte et la précision de l'apprentissage, tandis que les traits pleins représentent la perte et la précision de la validation. \n",
    "\n",
    "Pour combler ce problème, on pourrait réduire le nombre d'epocs. Essayez de reconstruire un réseau avec 4 epocs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Utiliser un réseau pour générer des prévisions sur de nouvelles données\n",
    "\n",
    "Après avoir formé un réseau, vous souhaiterez l’utiliser de manière pratique. Vous pouvez générer la probabilité que les avis soient positifs en utilisant la méthode `Predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# model.predict(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Autres expériences\n",
    "\n",
    "\n",
    "* Nous utilisions 2 couches cachées. Essayez d'utiliser 1 ou 3 couches cachées et voyez comment cela affecte la validation et la précision des tests.\n",
    "* Essayez d'utiliser des couches avec plus d'unités cachées ou moins: 32 unités, 64 unités ...\n",
    "* Essayez d’utiliser la fonction de perte `mse` au lieu de` binary_crossentropy`.\n",
    "* Essayez d'utiliser l'activation `tanh` (une activation qui était populaire dans les premiers jours des réseaux de neurones) au lieu de` relu`."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
